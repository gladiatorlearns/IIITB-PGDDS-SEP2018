Introduction

Welcome to the second session of PCA. In the previous session, you discovered and learnt the theoretical concepts of PCA. In this session, you will learn how to implement PCA in python on some real examples.

 

In this session, you will learn how to use PCA on a problem you have already encountered before - predicting telecom churn using logistic regression. You will now learn to implement PCA in tandem with logistic regression.

 

Before you move ahead, it is recommended that you quickly go through the notebook once and get yourself familiar with the dataset etc. You can download the notebook and the datasets from the bottom of this page.

Prerequisites

There are no prerequisites for this session other than the knowledge of the previous session.

 

Guidelines for in-module questions

The in-video and in-content questions for this module are not graded. The graded questions are given in a separate segment labelled 'Graded Questions' at the end of the session. The questions in that segment will adhere to the following guidelines:

 
 	First Attempt Marks	Second Attempt Marks

Question

with 2 Attempts
	10	5

Question

with 1 Attempt
	10 	0

 
People you will hear from in this session 

Subject Matter Expert

Mirza Rahim Baig

Lead Business Analyst, Flipkart

 
Downloads
IPython Notebook
file_downloadDownload
internet.csv
file_downloadDownload
customer_data.csv
file_downloadDownload
churn_data.csv
file_downloadDownload


PCA: Python Implementation

You will now learn to implement PCA in python. Please download the Jupyter notebook and the datasets given below and implement the code along with the lecture.

 

Ipython Notebook

Logistic Regression - Telecom Churn with PCA
file_downloadDownload

 

Datasets
Advertising
file_downloadDownload
churn_data
file_downloadDownload
customer_data
file_downloadDownload
internet_data
file_downloadDownload

First, let's quickly recall how we had implemented logistic regression on the telecom churn problem. This lecture will quickly walk you through the entire process of building a logistic regression model.

 

Video Correction: At 03:52, Rahim says 'linear regression' though he meant 'logistic regression'. 
Current Time 0:25
/
Duration 8:54
 
Playback Rate
Auto
1.5x

You saw the process of building a churn prediction model using logistic regression. Some important problems with this process that Rahim pointed out are:

    Multicollinearity among a large number of variables, which is not totally avoided even after reducing variables using RFE (or a similar technique)
    Need to use a lengthy iterative procedure, i.e. identifying collinear variables, using variable selection techniques, dropping insignificant ones etc.
    A potential loss of information due to dropping variables
    Model instability due to multicollinearity

 

Now, let's take a look at how PCA can be used to solve these problems.

With PCA, you could achieve so much with just a couple of lines of code. PCA helped us solve the problem of multicollinearity (and thus model instability), loss of information due to dropping variables, and we don't need to use iterative feature selection procedures.

 

Now, let's see how you can use PCA to automatically choose the principal components which explain x% of the variance. You'll also learn some other tips and tricks to work with PCA.

This brings us to the end of this segment. In the next segment, you will get to know more about the practical considerations of PCA and also its alternatives to overcome PCA's shortcomings.


Practical Considerations and Alternatives

Let's now look at some practical considerations that need to be kept in mind while applying PCA.

Those were some important points to remember while using PCA. To summarise:

    Most software packages use SVD to compute the components and assume that the data is scaled and centred, so it is important to do standardisation/normalisation
    PCA is a linear transformation method and works well in tandem with linear models such as linear regression, logistic regression etc., though it can be used for computational efficiency with non-linear models as well
    It should not be used forcefully to reduce dimensionality (when the features are not correlated)

 



You learnt some important shortcomings of PCA:

    PCA is limited to linearity, though we can use non-linear techniques such as t-SNE as well (you can read more about t-SNE in the optional reading material below)
    PCA needs the components to be perpendicular, though in some cases, that may not be the best solution. The alternative technique is to use Independent Components Analysis 
    PCA assumes that columns with low variance are not useful, which might not be true in prediction setups (especially classification problem with class imbalance)

 

If you are interested in reading about t-SNE (t-Distributed Stochastic Neighbor Embedding) or ICA, you can go through the additional reading provided below.

 

This brings us to the end of this segment. And now, you will see your optional assignment in

the next segment.

 
Additional Reading 

t-SNE

    Laurens van der Maaten's (creator of t-SNE) website https://lvdmaaten.github.io/tsne/
    Visualising data using t-SNE: Journal of Machine Learning Research http://www.jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf
    How to use t-SNE effectively  https://distill.pub/2016/misread-tsne/

 

Independent Components Analysis

    Stanford notes on ICA  http://cs229.stanford.edu/notes/cs229-notes11.pdf
In the next short lecture, Rahim will talk about some shortcomings of PCA. 


Optional Assignment (MNIST Dataset)

In this optional practice assignment, you can try your hands on applying PCA to a supervised classification problem.

 

You will use the popular MNIST handwritten digits dataset and build a classifier to predict the label of a given digit. Rahim will quickly walk you through the problem statement.


Dataset

The dataset consists of images of handwritten numeric digits between 0-9. Each image is of 28 x 28 pixels, i.e. 28 pixels along both length and breadth of the image. Each pixel is an attribute with a numeric value (representing the intensity of the pixel), and thus, there are 784 attributes in the dataset.

 

You can download the dataset from Kaggle here.

https://www.kaggle.com/c/digit-recognizer/data
 

Problem Statement

The task is to build a classifier that predicts the label of an image (a digit between 0-9) given the features. Thus, this is a 10-class classification problem. Since this dataset has a large number of features (784), you can use PCA to reduce the dimensionality, and then, build a model on the low-dimensional data.

 

An informative workflow is mentioned below:

    Classifier without PCA: If you haven’t attempted the optional SVM assignment, first try to build a model on the original 784 attributes. You may try using logistic regression, Naive Bayes, SVM (linear and non-linear), random forests, and so on.

        Tune the hyperparameters of your model using an appropriate method. Which evaluation metric would you choose to measure the model performance?

        Which model would perform the best?

 

    Classifier with PCA: Now, try to reduce the dimensionality of the dataset from 784 to a lower number k, and build a model of your choice (ideally chosen based on step-1 above) on the lower dimension data. Here, you’ll need to think about the following uncertainties:

        How would you find the optimal value of k? Try experimenting with various values of k, build and tune the classifier with various values of k, and compare the model performance at various values of k with the model hyperparameters.

        Notice that now you are treating k as a ‘model’ hyperparameter along with the classifier’s hyperparameters, where now your ‘composite model’ has two models in sequence — PCA and a classifier.

            How would you tune the optimal value of k and the classifier’s hyperparameters simultaneously? For example, if you are building an SVM with a radial kernel, you need to tune C and gamma. Can you also tune k just like you tuned the other hyperparameters?

                Hint: Try using the Pipeline feature of sklearn to chain the two models and tune the hyperparameters using GridSearchCV. 
https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html
 

You can use the following rubrics to self-evaluate your solution. 

 

Self-evaluation Rubrics

 

Criteria
	

 
	

 
	

Meets Expectations
	

 
	

Doesn't Meet Expectations

 

Data Understanding, Preparation, and EDA
	

 

Relevant data quality checks are performed, and all data quality issues are addressed in the right way (missing value imputation, removing duplicate data, and other kinds of data redundancies if any).

 

Data is not scaled appropriately before applying PCA.
	

 

All the quality checks are not done, and data quality issues are not addressed correctly up to an appropriate level.


 

Data is not scaled appropriately.
Model Building and Evaluation	

Model parameters are tuned using correct principles, and the approach is explained clearly.

 

Model evaluation is done using the correct principles, and appropriate evaluation metrics are chosen.

 

Optimal hyperparameters are correctly chosen using cross-validation.

 

Pipelining of PCA and the classifier is done correctly, and the optimal 'n_components' and hyperparameters are chosen.

 

The results are on par with the best possible model in the dataset.  
	

 

Model parameters are not tuned using the correct principles, and the approach is not explained clearly.

 

Model evaluation is not done using the correct principles, and appropriate evaluation metrics are not chosen.

 

Optimal hyperparameters are incorrectly chosen using cross-validation (e.g. using incorrect metrics/training data rather than test).

 

The results are not on par with the best possible model in the dataset.

 
Conciseness and Readability of Code	

 

The code is concise and syntactically correct. Wherever appropriate, built-in functions and standard libraries are used instead of writing long code snippets (containing if-else statements, for loops, and so on).

 

Custom functions are used to perform repetitive tasks.

 

The code is readable with appropriately named variables, and detailed comments are provided wherever necessary.
	

 

Long, complex code snippets are used instead of shorter built-in functions.

 

Custom functions are not used to perform repetitive tasks, which results in the repeated execution of the same piece of code.

 

Code readability is poor because of vaguely named variables or a lack of comments where they are necessary.


Comprehension: PCA, SVD and Eigenvectors

Now that you have learnt the key concepts of PCA, let's look at PCA from another point of view. You have seen that SVD is used to compute the principal components, but there's an alternative technique to do that as well - using eigenvectors. 

 

Please go through the handout attached below.

The Relation Between SVD, PCA and Eigenvectors
file_downloadDownload

Now, solve the following questions:


Questions:1/6
 
 
 
 
 
 
Eigenvector

If a matrix operates on a vector and the vector doesn't change its span, then the vector is an eigenvector of the said matrix. Which of the following options is an eigenvector of the matrix below?

 

(1, 2)
Feedback :

Multiplying vector (1,2) with the given vector produces (2 x 1+3 x 2 = 8, 4 x 1+6 x 2 = 16) = (8,16) = 8 times (1,2). As the span of the vector remains the same, it is an eigenvector.

Correct!

(2, 1)

(3, 2)

(2, 3)
Incorrect!

Questions:2/6
 
 
 
 
 
 
Eigenvalue

If the span of a vector remains the same after the matrix operation, and there's a change in the magnitude of the vector, then the ratio of the original magnitude to the new magnitude is called the eigenvalue. In other words, the factor by which a vector is enlarged or squashed on its span is called eigenvalue. If one of the eigenvectors of the matrix is , what is the eigenvalue?

3

5
Feedback :

Multiplying (2,1) with the given matrix gives (2 x 2+6 x 1 = 10, 1 x 2 + 3 x 1 = 5) = (10, 5) = 5 times (2,1). Hence, the given vector gets enlarged by 5 times; therefore, the eigenvalue is 5.
Correct!

2

1


Questions:3/6
 
 
 
 
 
 
PCA

Consider the following picture. A square of two unit lengths is drawn at the centre of a two-dimensional subspace defined by the basis i (1,0) and j (0,1). The space is vertically scaled, which transforms the square into a rectangle with double the height of that of the square, though the width remains the same.

Answer the following questions based on the facts above.

Which of the three vectors will not be pointing in the same direction after a vertical scaling?

(0, 1)

(1, 1)
Feedback :

The slope of the diagonal vector changes from 1 to 2.
Correct!

(1, 0)

None of the above

Questions:4/6
 
 
 
 
 
 
SVD

SVD and PCA are related to each other as:

SVD is an analogous data analysis technique to PCA and these two are often used alternately

SVD is a computational technique used to find the principal components
Feedback :

Yes, it does that by decomposing the data matrix into U, S, VT
Correct!

PCA cannot be done without SVD

None of these


Questions:5/6
 
 
 
 
 
 
Co variance Matrix

If the original data matrix A is of dimensions n x p, i.e. n observations and p features, then the covariance matrix of A (choose all correct options):

Is an n x n square matrix
Feedback :

The covariance matrix contains variances of original columns along the diagonal and covariances at the off-diagonal entries.
Incorrect

Is a p x p square matrix
Feedback :

The covariance matrix contains p rows and p columns with each diagonal entry being the variance of a column and the off-diagonal entries being the covariances. Since cov(x, y) = cov(y, x), the element at index (i,j) and (j, i) are the same, and hence the matrix is symmetric.
CorrectYou missed this!

Is a symmetric matrix
Feedback :

The covariance matrix contains p rows and p columns with each diagonal entry being the variance of a column and the off-diagonal entries being the covariances. Since cov(x, y) = cov(y, x), the element at index (i,j) and (j, i) are the same, and hence the matrix is symmetric.
CorrectYou missed this!

Is a diagonal matrix

Contains variances of original columns along the diagonal
Feedback :

The covariance matrix contains p rows and p columns with each diagonal entry being the variance of a column and the off-diagonal entries being the covariances. Since cov(x, y) = cov(y, x), the element at index (i,j) and (j, i) are the same, and hence the matrix is symmetric.
CorrectYou missed this!

Contains covariances among the original columns at the off-diagonal elements
Feedback :

The covariance matrix contains variances of original columns along the diagonal and covariances at the off-diagonal entries.

Questions:6/6
 
 
 
 
 
 
PCA

More than one options may be correct. While conducting PCA of an n x p data matrix A:

SVD decomposes the covariance matrix of A into U, S and VT

Eigenvectors of the matrix A form the principal components
Feedback :

No, eigenvectors of the covariance matrix of A are computed, not A itself
Incorrect

Eigenvectors of the covariance matrix of A form the principal components
Feedback :

Yes, in eigenvalue decomposition, the covariance matrices eigenvectors are the PCs.
Correct

SVD decomposes the matrix of into U, S and VT where the column vectors of V are the principal component vectors
Feedback :

Yes, the matrix V’s columns basically contain the principal component vectors.  


 

Consider the following picture. A  square of two unit lengths is drawn at the centre of a two-dimensional subspace defined by the basis i (1,0) and j (0,1). The space is vertically scaled, which transforms the square into a rectangle with double the height of that of the square, though the width remains the same.

 

 

Answer the following questions based on the facts above.


Questions:1/3
 
 
 
Eigenvector

Which of the three vectors will not change its magnitude after this transformation of space?

(0, 1)

(1, 1)

(1, 0)
Correct!

None of the above


Questions:2/3
 
 
 
Eigenvector

Eigenvectors are characteristics of any such transformation. The vectors that don't change their directions are the eigenvectors of the transformation. Which of the three vectors is not an eigenvector of this transformation?

(0, 1)

(1,1)
Feedback :

Point (1, 1) becomes (1, 2) after the transformation. As the span of this vector gets changed, it's not an eigenvector.
Correct!

(1, 0)

None of the above


Questions:3/3
 
 
 
Eigenvalue

Eigenvalue is the factor by which the magnitude of a vector changes. What is the eigenvalue for the transformation of the vertical black vector?

2
Feedback :

It's clearly evident in the diagram that the vector gets doubled, and hence, the eigenvalue is 2.
Correct!

1

3

4
 





