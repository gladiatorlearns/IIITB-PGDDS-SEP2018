You will now learn an important technique used to compute the principal components, called singular value decomposition or SVD.

 

Before you get into the details of SVD, wouldn’t it be nice to get a quick recap of the basics and the important terms of vectors and matrices?

 

The document attached below will help you recall the common terminologies of matrices, some of which is used in the following lectures. Please go through it if you think you are not totally comfortable with matrix transpose, eigenvectors and eigenvalues, singular matrices etc.

 

In the following lecture, Rahim explains the various methods used to find the principal components.


Matrices - Quick Refresher
file_downloadDownload

In the following lecture, you will see an example of SVD to concretize the concepts you have just learnt:

To summarise, SVD helps you decompose an m x n matrix A into three component matrices - U, S and V' as shown below. 
SVD
Comprehension - PCA and SVD

You saw that if each row in the original dataset A (m x n) represents a user and each column an item such as a book, then:

    The matrix U (m x k) maps the users to the new k themes/latent variables
    The diagonal matrix S (k x k) represents the strength of each feature
     Matrix V' (k x n) maps each of the k themes/latent variables to the original n features.

 

If you choose k < n, i.e. k principal components, then you essentially achieve dimensionality reduction, since you have now reduced an m x n dataset with n original features to an m x k dataset with k principal components. 

 

In the previous segment, we had mentioned that the coefficients of the ith principal component vector ϕi= ϕ1i,ϕ2i,....ϕni (an n-dimensional vector) are called the loadings, while the scores along the ith component are represented as:  

 

zi=ϕ1ix1+ϕ2ix2+....+ϕnixn,

 

The matrix Us, i.e. the product of the first two matrices given by SVD, is an m x k matrix which contains the scores z - each of the m rows contains the score of an observation along the kth principal component. 

 

Also, the matrix VT, which is a k x n matrix, contains the loadings ϕi,j where each of the k rows represents a principal component vector ϕi. In other words, each row of this matrix represents an n-dimensional principal component vector.

 

Thus, one can interpret the output of SVD as:

 

A=Us  VT=Scores.Loadings
 
 Questions:1/2
 
 
SVD and Dimensionality Reduction

SVD is used to decompose an m x n matrix A into the component matrices U (m x k), S (k x k) and V' (k x n). Most packages (e.g. in sklearn) use SVD to compute the principal component loadings and scores.

The process will be called dimensionality reduction when:   

k < m

k < n
Feedback :

The k dimensions in the new space represent the new 'feature space', and when k < n, n being the original number of columns, we call it dimensionality reduction.
Correct!

n < k

Questions:2/2
 
 
Principal Component Vectors

SVD decomposes an m x n matrix A into the component matrices U (m x k), S (k x k) and V' (k x n). Each principal component vector is represented as ϕ=(ϕ1,ϕ2,...ϕp). The dimensionality of this vector is:

k

n
Feedback :

Principal component vectors are of the same dimension as the original space, i.e. n. Each component's loadings are multiplied with the respective features to get the score of that component, i.e. z=ϕ1x1+ϕ2x2+...+ϕnxn
Correct!

m


SVD Example - Image Compression

Now that you have a good intuition of SVD, let's look at a common useful application of SVD - image compression.

Now that you have understood the basic technique used to compute the principal components, a natural question is how do you decide the number of principal components k.

 

That’s what the next lecture is all about. It talks about the intuition that helps in choosing the 'optimal' number of principal components using what is called a scree plot.


To summarize, the choice of the optimal number of principal components is made using the scree plot - the y-axis represents the cumulative variance captured while the x-axis represents the number of principal components. 


Questions:1/3
 
 
 
SVD

Which of the following is not true about singular value decomposition?

Only a square matrix can be decomposed using SVD
Feedback :

Matrices of any shape can be decomposed using SVD. In the examples used in the lecture, the original dataset A was of m x n dimensions, where m and n can be any integers.
Correct!

U is a matrix with as many rows as the number of data points
Incorrect!

Σ is a diagonal matrix with the strength of the themes that are arranged in decreasing order.


VT is a matrix with as many columns as the number of original dimensions


 Questions:2/3
 
 
 
Scree Plot

Which of the following is not true about scree plots?

Scree plotting is a technique to find out the optimum number of principal components

It is a plot between the number of PC as X-axis and the percentage (%) of variance captured as Y-axis

As we move right on to the X-axis, the percentage (%) of the total variance captured increases

The scree plot recommends a specific, unique value of the number of PCs 
Feedback :

It produces a visual relation between the number of PCs chosen and the percentage (%) of the cumulative variance captured. It does not give one unique value. Depending upon the problem at hand, you can choose how much variance captured is suitable.

Questions:3/3
 
 
 
Scree Plot

In the following scree plot, how much total information is retained using only three principal components?

More than 95%

Exactly 60%

Can't be gauged

More than 75%
Feedback :

Draw a line perpendicular to the X-axis from the middle of PC3. The point where it intersects the scree plot is the cumulative variance captured.
Correct!

 
 

 

