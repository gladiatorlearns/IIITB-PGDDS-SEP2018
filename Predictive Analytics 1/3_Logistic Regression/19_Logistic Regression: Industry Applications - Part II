Introduction

Welcome to the second session on 'Industry Applications of Logistic Regression'.

 

In the previous session, you learnt how to prepare data for modelling by performing tasks such as sample selection, segmentation, and variable transformation techniques. 

 
In this session

You will learn how the models are evaluated and governed in the industries. Broadly speaking, the agenda for the session is as follows:

    Model Evaluation (Performance) Measures
        Discriminatory power
        Accuracy 
        Stability
    Model Validation
        In-sample validation
        Out-time validation
        K-cross validation
    Model tracking or Model governance
    Model recalibration  

 

Note: This is an introductory session on model validation. Don't worry if you don't understand this completely. You will be taught model validation again in detail in the PA-II  course.

 
Guidelines for in-session questions

The in-video and in-content questions for this module are not graded. 

 
People you will hear from in this session

Subject Matter Expert

Hindol Basu

Partner, Analytics Division at Tata (Tata IQ)

Tata iQ is one of the leading analytics services providers in India and is a part of the prestigious Tata Group.  Mr Hindol leads the analytics practices at Tata.


Let's look at 2 cases, where the variable's value is missing (equal to NA):

    Utilisation is missing: As mentioned earlier, if this variable is missing for a particular customer, that could very well be because the bank did not find that customer worthy enough for a credit card. Hence, these missing values are not missing at random, and it would be unfair to just replace them with the mean or the median. As mentioned earlier, it would be wiser to perform a WOE analysis and then replace these values.
    Age is missing: Consider why the variable age is missing for some customers. Here, it may actually make more sense to just replace the missing value with the mean or the median, instead of wasting time on WOE analysis. This is because it is very likely that the variable age is just missing because of a system error or a manual error, and there is no clear pattern behind the missing values.

 

Also, recall that Hindol mentioned that there are two more methods for treating missing values. Those were not taught in this course because of their complexity. However, if you wish to, you can go through both of them here:

    Markov Chain Monte Carlo - SAS Support https://support.sas.com/resources/papers/proceedings13/436-2013.pdf
    Expectation Maximisation - Oxford University Statistics http://www.stats.ox.ac.uk/~steffen/teaching/fsmHT07/fsm407c.pdf
    
    Model Evaluation (A Second Look)

In a previous session on model evaluation, you learnt about various model evaluation measures, such as accuracy, sensitivity, specificity, KS statistic, etc. Now, let's look at some more measures commonly used for model evaluation.


Note: At 02:29 our SME's says "%Goods in X-axis and %Bad in Y-axis", please note that it should be %Bad in X-axis(FPR) and %Good in Y-axis(TPR).

 

Hindol also talked about some measure other than sensitivity and specificity. If you want to go deep and learn about them, you can access them through the following links:

    KS Statistic https://learn.upgrad.com/v/course/209/session/22196/segment/116851
    Rank Ordering https://www.listendata.com/2015/01/model-validation-in-logistic-regression.html

 

Following shows the ROC curve for 3 cases, i.e. the random model (orange line), the ideal model (grey line), and the real model(blue line), in our case, the telecom churn model.
ROC Curve for Telecom Churn



 

Clearly, the perfect model is pretty much a right triangle, whereas the random model is a straight line. Basically, a model that rises steeply is a good model.

 

Another way of saying that is — it will have a higher area under the curve. So, the Gini coefficient, which is nothing but a fancy word and is given by -

 

Gini=AreaUnderROCCurve

 

will be high for a good model.

Model Validation and Importance of Stability

So, you've seen how a model is evaluated, using various parameters such as accuracy, sensitivity, KS statistic, gini coefficient, etc. However, so far you've only tested models on data that was formed after splitting one data set into training data and testing data. Is that enough? Let's see.

Let's again look at the telecom churn example from before. The data used to build the model was from 2014.

 

You split the original data into two parts, i.e. training and testing data. However, these two parts were both built with data from 2014.
Training and Testing Split

This is called in-sample validation. Testing your model on this test data may not be enough though, as the test data here is too similar to training data.

 

So, it makes sense to actually test the model on data that is from some other time, like 2016. This is called out-of-time validation.
Out of Time Validation

Another way to do the same thing is to use K-fold cross validation. Basically, the evaluation of the sample is done for k-iterations. E.g. here's a representation of how 3-fold cross validation works:
3 Fold Cross Validation

Basically, there are 3 iterations in which evaluation is done. In the first iteration, 1/3rd of the data is selected as training data and the remaining 2/3rd of it is selected as testing data. In the next iteration, a different 1/3rd of the data is selected as the training data set and then the model is built and evaluated. Similarly, the third iteration is completed.

 

Such an approach is necessary if the data you have for model building is very small, i.e. has very few data points.

 

If these three methods of validation are still unclear to you, you need not worry as of now. They will be covered at length in the module on Model Selection.

Note for 0:43 to 2:07: While explaining how WOE is linked to stability and PSI, Hindol used IV values mistakenly. He meant to use WOE values.

 

Obviously, a good model will be stable. A model is considered stable if it has:

    Performance Stability: Results of in-sample validation approximately match those of out-of-time validation
    Variable Stability: The sample used for model building hasn't changed too much and has the same general characteristics
    
    Tracking of Model Performance Over Time

Now, suppose you conclude, based on the previously mentioned criteria, that the model is not stable. What will you do then? Let's hear from Hindol on that.

Let's go back to the telecom churn example. If you recall, the model was built using data from 2014. Now suppose, you are tracking its performance over time, and that ends up giving you the following results:
Model Performance Over Time

So, the first time, when the model's Gini dropped to 0.72, you avoided building a new model. Basically, you just recalibrated, i.e. updated the coefficients of the variables. That resulted in a slight increase of Gini. However, the next time Gini dropped to a low value, i.e. 0.71, we just rebuilt the model, i.e. got new sample data, performed data prep, etc. and built the entire model.

Let's go back to the telecom churn example. If you recall, the model was built using data from 2014. Now suppose, you are tracking its performance over time, and that ends up giving you the following results:
Model Performance Over Time

So, the first time, when the model's Gini dropped to 0.72, you avoided building a new model. Basically, you just recalibrated, i.e. updated the coefficients of the variables. That resulted in a slight increase of Gini. However, the next time Gini dropped to a low value, i.e. 0.71, we just rebuilt the model, i.e. got new sample data, performed data prep, etc. and built the entire model.

Summary

In this session, you learnt about various model evaluation measures, such as accuracy, sensitivity, specificity, KS statistic, etc as shown in figure 1. Then, you learnt how to validate your model on in-sample data and out-time data (unseen data). Also, for stability, you learnt and understood stability by two metrics — Variable Distributive Stability and Predictive Pattern Stability. Then, you learnt about model tracking and model recalibrating steps in detail.

You can also download the lecture notes for the logistic regression module below.

https://cdn.upgrad.com/UpGrad/temp/c1155022-b004-4aee-a730-183b6f0fcaff/Logistic+Regression+Lecture+Notes.pdf
