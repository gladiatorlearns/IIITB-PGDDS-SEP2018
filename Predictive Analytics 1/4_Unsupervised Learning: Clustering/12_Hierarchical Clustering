Introduction

Welcome to the session on 'Hierarchical Clustering'. In the previous sessions, you got a basic understanding of what clustering is and how you can use the K-Means algorithm to create clusters in your data set. You also saw the execution of the K-Means algorithm in Python.

 

In this session

You will learn about another algorithm to achieve unsupervised clustering. This is called Hierarchical Clustering. Here, instead of pre-defining the number of clusters, you first have to visually describe the similarity or dissimilarity between the different data points and then decide the appropriate number of clusters on the basis of these similarities or dissimilarities.

 

You will learn about:

    Hierarchical clustering algorithm
    Interpreting the dendrogram
    Cutting the dendrogram
    Types of linkages
    
    
Hierarchical Clustering Algorithm

One of the major considerations in using the K-means algorithm is deciding the value of K beforehand. The hierarchical clustering algorithm does not have this restriction.

 

The output of the hierarchical clustering algorithm is quite different from the K-mean algorithm as well. It results in an inverted tree-shaped structure, called the dendrogram. An example of a dendrogram is shown below.

Dendrogram

In the K-Means algorithm, you divided the data in the first step itself. In the subsequent steps, you refined our clusters to get the most optimal grouping. In hierarchical clustering, the data is not partitioned into a particular cluster in a single step. Instead, a series of partitions/merges take place, which may run from a single cluster containing all objects to n clusters that each contain a single object or vice-versa.

 

This is very helpful since you don’t have to specify the number of clusters beforehand.

 

Given a set of N items to be clustered, the steps in hierarchical clustering are:

    Calculate the NxN distance (similarity) matrix, which calculates the distance of each data point from the other

    Each item is first assigned to its own cluster, i.e. N clusters are formed

    The clusters which are closest to each other are merged to form a single cluster

    The same step of computing the distance and merging the closest clusters is repeated till all the points become part of a single cluster

 

Thus, what you have at the end is the dendrogram, which shows you which data points group together in which cluster at what distance. You will learn more about interpreting the dendrogram in the next segment.

 

Look at the image given below and answer the question that follows.
Clustering Image


Questions:1/2
 
 
Hierarchical Clustering

You had made clusters for it using the K-Means algorithm. How do you think clusters will be made using hierarchical algorithm on this data?
! Note: Once submitted, answer is not editable.
lightbulb_outline

Suggested Answer

Since now you are looking at the closest distance between clusters, you will get two clusters - one at the center and the one which contains the points at the edges.

Questions:2/2
 
 
Hierarchical Clustering

Look at the following matrix. This is the distance matrix between 4 points - A, B,C, D. Find out which 2 clusters will merge first.
 	A	B	C	D
A	 	 	 	 
B	2.24	 	 	 
C	8.06	10.00	 	 
D	5.83	8.06	5.00	 
A-B
Feedback :
Look at the data and see that the minimum distance is between A and B
Correct!
B-C
C-D
C-A


Interpreting the Dendrogram

The result of the cluster analysis is shown by a dendrogram, which starts with all the data points as separate cluster and indicates at what level of dissimilarity any two clusters were joined.

As you saw, the y-axis of the dendrogram is some measure of the dissimilarity or distance at which clusters join.
Fig 1: Dendrogram

 

In the dendrogram shown above, samples 4 and 5 are the most similar and join to form the first cluster, followed by samples 1 and 10. The last two clusters to fuse together to form the final single cluster are 3-6 and 4-5-2-7-1-10-9-8. 

 

Determining the number of groups in a cluster analysis is often the primary goal. Typically, one looks for natural groupings defined by long stems. Here, by observation, you can identify that there are 3 major groupings: 3-6, 4-5-2-7 and 1-10-9-8.

 

You also saw that hierarchical clustering can proceed in 2 ways — agglomerative and divisive. If you start with n distinct clusters and iteratively reach to a point where you have only 1 cluster in the end, it is called agglomerative clustering. On the other hand, if you start with 1 big cluster and subsequently keep on partitioning this cluster to reach n clusters, each containing 1 element, it is called divisive clustering.
Fig 2: Types of Hierarchical Clustering
Additional Reference

You can read more about divisive clustering here and here.

 https://nlp.stanford.edu/IR-book/html/htmledition/divisive-clustering-1.html
 http://luthuli.cs.uiuc.edu/~daf/courses/probcourse/notesclustering.pdf
 

Comprehension - Hierarchical Clustering Algorithm

Given below are five data points having two attributes x and y:

 
Observation	x	y
1	3	2
2	3	5
3	5	3
4	6	4
5	6	7

 

The distance matrix of the points, indicating the Euclidean distance between points, is as follows:

 
Label	1	2	3	4	5
1	0.00	3.00	2.24	3.61	5.83
2	3.00	0.00	2.83	3.16	3.61
3	2.24	2.83	0.00	1.41	4.12
4	3.61	3.16	1.41	0.00	3.00
5	5.83	3.61	4.12	3.00	0.00

 

Take the distance between two clusters as the minimum distance between the points in the two clusters. Based on this information, answer the following questions.

 

Questions:1/6
 
 
 
 
 
 
Hierarchical Clustering

How many clusters are there initially (before any fusions have happened)?
None
1
4
5
Feedback :
Since this is agglomerative clustering, initially, all the points are 1 cluster.
Correct!

Questions:2/6
 
 
 
 
 
 
Hierarchical Clustering

Which two clusters will be fused first?
1 and 2
3 and 4
Feedback :
These two points(clusters) have the minimum distance.
Correct!
3 and 5
1 and 5

Questions:3/6
 
 
 
 
 
 
Hierarchical Clustering

Which clusters will be fused in step two?
1 will be fused with 2
2 will be fused with 5
1 will be fused with the cluster(3, 4)
Feedback :
The distance between points 1 and 3 is 2.24 units, which is the minimum among all the new clusters. Hence they will be joined now.


Questions:4/6
 
 
 
 
 
 
Hierarchical Clustering

How many total clusters are there right after point number 1 fuses with the cluster(3, 4)?
3
Feedback :
Since three points have fused into 1 cluster, total clusters left are (1,3,4) - (2) - (5).
Correct!
4
2
5

Questions:5/6
 
 
 
 
 
 
Hierarchical Clustering

Which clusters will be fused after 1 fuses with (3, 4)?
2 and 5 will fuse with each other
5 will fuse with the cluster (1, 3, 4)
2 will fuse with the cluster (1, 3, 4)
Feedback :
The distance of point 2 from point 3 is 2.83 units, whereas the minimum distance of point 5 from the cluster (1,3,4) is 3 units.

Questions:6/6
 
 
 
 
 
 
Hierarchical Clustering

What happens in the last step of the algorithm?
1 fuses with (2, 3, 4, 5)
5 fuses with ( 1, 2, 3, 4)
Feedback :
Since all the other points are already part of one cluster, the last point will also join that cluster at this step.
Correct!
2 fuses with (1, 3, 4, 5)
4 fuses with (1, 2, 3, 5)


Types of Linkages

In our example, we took the minimum of all the pairwise distances between the data points as the representative of the distance between 2 clusters. This measure of the distance is called single linkage. Apart from using the minimum, you can use other methods to compute the distance between the clusters.


Let’s see once again the different types of linkages.

    Single Linkage: Here, the distance between 2 clusters is defined as the shortest distance between points in the two clusters
    Complete Linkage: Here, the distance between 2 clusters is defined as the maximum distance between any 2 points in the clusters
    Average Linkage: Here, the distance between 2 clusters is defined as the average distance between every point of one cluster to every other point of the other cluster.

 

You have to decide what type of linkage should be used by looking at the data. One convenient way to decide is to look at how the dendrogram looks. Usually, single linkage type will produce dendrograms which are not structured properly, whereas complete or average linkage will produce clusters which have a proper tree-like structure. You will see later what this means when you run the hierarchical clustering algorithm in Python.

 
Additional reading

You can read more about the type of linkages here, here and here.

 

Use the excel file give below to answer the questions that follow:

Questions:1/4
 
 
 
 
Hierarchical Clustering

Select the appropriate option which describes the Complete Linkage method.
In complete linkage hierarchical clustering, the inter cluster distance is defined as the shortest distance between two points (one point in each cluster).
Feedback :
Don’t get confused between how a cluster is merged with another cluster and how the distance on the basis of which the merging happens in calculated.
Incorrect!
In complete linkage hierarchical clustering, the inter cluster distance is defined as the longest distance between two points (one point in each cluster)
Feedback :
In the complete linkage, inter cluster distance is calculated as the maximum distance between 2 points (one in each cluster), However, the point is assigned to a new cluster basis it’s minimum distance from the clusters
Correct!
In complete linkage hierarchical clustering, the inter cluster distance is defined as the average distance between two points (one point in each cluster)

Questions:2/4
 
 
 
 
Hierarchical Clustering

Select the points which get clustered in the first iteration.(First iteration is defined as the first merging of clusters - ie, from n clusters to n-1 clusters)
A, B
Feedback :
You will select the points with the minimum distance between them amongst all the possible combinations.
Incorrect!
B, C
C, D
Feedback :
Look at the distance between 2 points. You will see that the minimum distance is 2.2 units, which is between C and D
Correct!
A, E


Questions:3/4
 
 
 
 
Hierarchical Clustering

Select the points which get clustered in the second iteration. Use single linkage method
A, B
A, F
B, E
Feedback :
What is the distance between this new cluster and every other cluster? Is it smaller than the distance between the clusters
Incorrect!
A, E
Feedback :
You can see that the minimum distance now is between point A and E, which is 3.2 units.

Questions:4/4
 
 
 
 
Hierarchical Clustering

How many iterations are required to form the final single cluster?
2
3
4
Feedback :
When there are 2 points, how many iterations does it take? When there are 3 points?
Incorrect!
5
Feedback :
Initially, n clusters are made and in each iteration, the number of clusters gets reduced by 1. So the number of iterations required is n-1. Here n = number of points = 6. So the correct answer is 5.


Let's recall what you have learnt in this session so far. You learnt about another clustering technique called Hierarchical clustering. You saw how it is different from K-Means clustering. One major advantage is that you do not have to pre-define the number of clusters. However, since you compute the distance of each point from every other point, it is time-consuming and needs a lot of processing power.

In the next segment, you will use the hierarchical clustering technique to actually make clusters using Python.
